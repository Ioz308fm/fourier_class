{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eabd490-0121-406f-b6a5-4e1ec9946cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_py import convert_to_polar, polar_conversion, func_def, fourier_coef_calc, get_label, img_plot, polar_conversion_fast, get_label_fast\n",
    "from functions_py import pipeline_train, batch_get_labels, optimized_test_class_alg\n",
    "from functions_py import optimized_func_def, optimized_fourier_coef_calc\n",
    "from functions_py import qwen_pipeline_train, qwen_optimized_test_class_alg, qwen_optimized_func_def, qwen_optimized_fourier_coef_calc, qwen_polar_conversion_fast\n",
    "from functions_py import run_hyperparameter_optimization\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from numba import njit\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b89ad500-6408-45dd-aa83-8d509882c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN определение\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def cnn_test(\n",
    "    df: pd.DataFrame,\n",
    "    preprocess_func: callable,\n",
    "    random_state: int = 42,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 128\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Тестирует CNN на предобработанных данных.\n",
    "    \n",
    "    Параметры:\n",
    "    - df: DataFrame с колонками 'label' и пикселями\n",
    "    - preprocess_func: функция предобработки, возвращает DataFrame с фичами\n",
    "    - random_state: для воспроизводимости\n",
    "    - epochs: количество эпох\n",
    "    - batch_size: размер батча\n",
    "    \n",
    "    Возвращает:\n",
    "    - (train_accuracy, test_accuracy)\n",
    "    \"\"\"\n",
    "    # Применяем предобработку\n",
    "    processed_df = preprocess_func(df)\n",
    "    \n",
    "    # Разделяем на признаки и метки\n",
    "    X = processed_df.drop('label', axis=1).values\n",
    "    y = processed_df['label'].values\n",
    "    \n",
    "    # Разбиваем на train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Нормализация (если нужно)\n",
    "    if X_train.max() > 1:\n",
    "        X_train = X_train.astype('float32') / 255.0\n",
    "        X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    # One-hot кодирование\n",
    "    lb = LabelBinarizer()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test = lb.transform(y_test)\n",
    "\n",
    "    # Определяем input_shape\n",
    "    input_shape = (X_train.shape[1], 1)  # (n_features, 1)\n",
    "    \n",
    "    # Архитектура сети\n",
    "    model = keras.Sequential([\n",
    "        layers.Reshape(input_shape, input_shape=(X_train.shape[1],)),\n",
    "        \n",
    "        # Conv1D вместо Conv2D, так как у нас 1D признаки\n",
    "        layers.Conv1D(32, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        \n",
    "        layers.Conv1D(64, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        \n",
    "        layers.Conv1D(128, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Обучение\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Оценка\n",
    "    train_acc = model.evaluate(X_train, y_train, verbose=0)[1]\n",
    "    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    \n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b069a494-7b8b-438e-a4f1-173dcc02f7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.96539\n",
      "Test Accuracy: 0.94821\n"
     ]
    }
   ],
   "source": [
    "# Тест CNN\n",
    "base = ''\n",
    "input_data = pd.read_csv(base + 'train.csv')\n",
    "state = 148\n",
    "\n",
    "train_acc, test_acc = cnn_test(\n",
    "    df=input_data,\n",
    "    preprocess_func=extract_autoencoder_features,  # Ваша функция\n",
    "    random_state=state,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.5f}\")  # ~0.995\n",
    "print(f\"Test Accuracy: {test_acc:.5f}\")    # ~0.990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6284b67-0a6a-44f2-9cbc-699a005e4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции предобработки\n",
    "def transform_pixels_to_sums_vectorized(df):\n",
    "    # Создаем копию исходного DataFrame\n",
    "    result_df = df[['label']].copy()\n",
    "    \n",
    "    # Получаем все пиксельные колонки\n",
    "    pixel_cols = [f'pixel{i}' for i in range(784)]\n",
    "    pixels = df[pixel_cols].values\n",
    "    \n",
    "    # Преобразуем в 3D массив (n_images, 28, 28)\n",
    "    images = pixels.reshape(-1, 28, 28)\n",
    "    \n",
    "    # Суммы по строкам (axis=2 - сумма по столбцам в каждом ряду)\n",
    "    row_sums = images.sum(axis=2)\n",
    "    \n",
    "    # Суммы по столбцам (axis=1 - сумма по рядам в каждом столбце)\n",
    "    col_sums = images.sum(axis=1)\n",
    "    \n",
    "    # Добавляем суммы в DataFrame\n",
    "    for i in range(28):\n",
    "        result_df[f'sum_row{i}'] = row_sums[:, i] / 28\n",
    "    for i in range(28):\n",
    "        result_df[f'sum_column{i}'] = col_sums[:, i] / 28\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "from skimage.feature import hog\n",
    "\n",
    "def extract_hog_features(df, orientations=8, pixels_per_cell=(8, 8), cells_per_block=(1, 1)):\n",
    "    result_df = df[['label']].copy()\n",
    "    pixel_cols = [f'pixel{i}' for i in range(784)]\n",
    "    images = df[pixel_cols].values.reshape(-1, 28, 28)\n",
    "    \n",
    "    hog_features = []\n",
    "    for image in images:\n",
    "        features = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                       cells_per_block=cells_per_block, feature_vector=True)\n",
    "        hog_features.append(features)\n",
    "    \n",
    "    hog_array = np.array(hog_features)\n",
    "    for i in range(hog_array.shape[1]):\n",
    "        result_df[f'hog_{i}'] = hog_array[:, i]\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def extract_statistical_features(df):\n",
    "    \"\"\"Извлечение статистических характеристик изображения\"\"\"\n",
    "    result_df = df[['label']].copy()\n",
    "    pixels = df[[f'pixel{i}' for i in range(784)]].values\n",
    "    \n",
    "    # Вычисление статистик\n",
    "    means = pixels.mean(axis=1)\n",
    "    stds = pixels.std(axis=1)\n",
    "    skews = skew(pixels, axis=1)\n",
    "    kurtoses = kurtosis(pixels, axis=1)\n",
    "    \n",
    "    result_df['mean'] = means\n",
    "    result_df['std'] = stds\n",
    "    result_df['skew'] = skews\n",
    "    result_df['kurtosis'] = kurtoses\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "from skimage.util import view_as_blocks\n",
    "\n",
    "def extract_pooling_features(df, pool_size=7, mode='max'):\n",
    "    \"\"\"Агрегация признаков по регионам изображения\"\"\"\n",
    "    result_df = df[['label']].copy()\n",
    "    pixels = df[[f'pixel{i}' for i in range(784)]].values\n",
    "    images = pixels.reshape(-1, 28, 28)\n",
    "    \n",
    "    assert 28 % pool_size == 0, \"Размер изображения должен делиться на размер окна\"\n",
    "    n_blocks = 28 // pool_size\n",
    "    \n",
    "    features = []\n",
    "    for img in images:\n",
    "        blocks = view_as_blocks(img, block_shape=(pool_size, pool_size))\n",
    "        if mode == 'max':\n",
    "            pooled = blocks.max(axis=(2,3)).flatten()\n",
    "        elif mode == 'mean':\n",
    "            pooled = blocks.mean(axis=(2,3)).flatten()\n",
    "        features.append(pooled)\n",
    "    \n",
    "    pooled_arr = np.array(features)\n",
    "    for i in range(pooled_arr.shape[1]):\n",
    "        result_df[f'pool_{mode}_{i}'] = pooled_arr[:, i]\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def extract_combined_features(df):\n",
    "    \"\"\"Комбинация HOG и статистических признаков\"\"\"\n",
    "    # Извлекаем HOG-признаки\n",
    "    hog_df = extract_hog_features(df)\n",
    "    \n",
    "    # Извлекаем статистические признаки\n",
    "    stat_df = extract_statistical_features(df).drop(columns=['label'])\n",
    "    \n",
    "    # Объединяем результаты\n",
    "    result_df = pd.concat([hog_df, stat_df], axis=1)\n",
    "    return result_df\n",
    "\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "def extract_lbp_features(df, radius=3, n_points=24):\n",
    "    result_df = df[['label']].copy()\n",
    "    pixels = df[[f'pixel{i}' for i in range(784)]].values\n",
    "    images = pixels.reshape(-1, 28, 28)\n",
    "    \n",
    "    lbp_features = []\n",
    "    for img in images:\n",
    "        lbp = local_binary_pattern(img, P=n_points, R=radius, method='uniform')\n",
    "        hist, _ = np.histogram(lbp, bins=n_points+2, range=(0, n_points+2))\n",
    "        lbp_features.append(hist)\n",
    "    \n",
    "    lbp_arr = np.array(lbp_features)\n",
    "    for i in range(lbp_arr.shape[1]):\n",
    "        result_df[f'lbp_{i}'] = lbp_arr[:, i]\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def extract_autoencoder_features(df, encoding_dim=32):\n",
    "    result_df = df[['label']].copy()\n",
    "    pixels = df[[f'pixel{i}' for i in range(784)]].values / 255.0\n",
    "    \n",
    "    # Архитектура автоэнкодера\n",
    "    input_img = Input(shape=(784,))\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "    decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    encoder = Model(input_img, encoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(pixels, pixels, epochs=10, batch_size=256, verbose=0)\n",
    "    \n",
    "    encoded_features = encoder.predict(pixels)\n",
    "    for i in range(encoding_dim):\n",
    "        result_df[f'ae_{i}'] = encoded_features[:, i]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdf02c65-4c54-4b23-bb61-ae9ab5175a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисление аккураси\n",
    "def test_train_class(predobr_func, D, M, T)\"\n",
    "    base = ''\n",
    "    input_data = pd.read_csv(base + 'train.csv')\n",
    "    conversion_state = True\n",
    "    write_state = False\n",
    "    state = 148\n",
    "    target = 'label'\n",
    "    data_with_labels = predobr_func(input_data)\n",
    "    \n",
    "    t_coef = T\n",
    "    file_temp_name = prefix + 'D'+ str(D) + 'M' + str(M)\n",
    "    \n",
    "    \n",
    "    data_train, data_test, data_train_means, fourier_coef = pipeline_train(data_with_labels, target, state, conversion_state, write_state, D, M)\n",
    "    print('train acc')\n",
    "    accuracy = optimized_test_class_alg(prefix, file_temp_name, target,data_train , data_train, data_train_means, fourier_coef, t_coef, rewrite = True)\n",
    "    print(accuracy)\n",
    "    print('test acc')\n",
    "    accuracy = optimized_test_class_alg(prefix, file_temp_name, target,data_test , data_train, data_train_means, fourier_coef, t_coef, rewrite = True)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "afd37efe-2eb3-4b90-a080-7f019adda94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "train acc\n",
      "0.674271499644634\n",
      "test acc\n",
      "0.6604617604617604\n"
     ]
    }
   ],
   "source": [
    "test_train_class(extract_autoencoder_features, 3400, 11, 1.4308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7d0a2-dee4-459d-b76e-63b8ddcba7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение\n",
    "base = ''\n",
    "input_data = pd.read_csv(base + 'train.csv')\n",
    "conversion_state = True\n",
    "write_state = False\n",
    "state = 148\n",
    "target = 'label'\n",
    "\n",
    "prefix = 'COEF_TEST_1.05_column_row_'\n",
    "D_left = 10\n",
    "D_right = 15000\n",
    "M_left = 2\n",
    "M_right = 21\n",
    "T_left = 0.1\n",
    "T_right = 2.0\n",
    "\n",
    "max_iter = 1000\n",
    "\n",
    "best_param, best_val = run_hyperparameter_optimization(extract_autoencoder_features, input_data, target, \n",
    "                                                       conversion_state, write_state, state, base, prefix, D_left, D_right, \n",
    "                                                       M_left, M_right, T_left, T_right, max_iter)\n",
    "print('param', best_param)\n",
    "print('acc = ', best_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
